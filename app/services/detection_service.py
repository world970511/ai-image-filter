"""
Layer 3: AI Detection Service
HuggingFace ëª¨ë¸ ê¸°ë°˜ AI ì´ë¯¸ì§€ íƒì§€
"""

import io
import asyncio
from typing import Dict, Any, Optional
from PIL import Image
import torch
from transformers import pipeline, AutoModelForImageClassification, AutoFeatureExtractor
from functools import lru_cache


class DetectionService:
    """AI ìƒì„± ì´ë¯¸ì§€ íƒì§€ ì„œë¹„ìŠ¤"""
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    AVAILABLE_MODELS = {
        "umm-maybe/AI-image-detector": {
            "description": "AI vs Real image classifier",
            "labels": {"artificial": "ai", "human": "real"}
        },
        "Organika/sdxl-detector": {
            "description": "SDXL generated image detector",
            "labels": {"artificial": "ai", "real": "real"}
        }
    }
    
    DEFAULT_MODEL = "umm-maybe/AI-image-detector"
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or self.DEFAULT_MODEL
        self._classifier = None
        self._model_loaded = False
    
    @property
    def classifier(self):
        """Lazy loading of the classifier"""
        if self._classifier is None:
            self._load_model()
        return self._classifier
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ í˜¸ì¶œ ì‹œ)"""
        try:
            print(f"ğŸ”„ Loading model: {self.model_name}")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            device = 0 if torch.cuda.is_available() else -1
            
            self._classifier = pipeline(
                "image-classification",
                model=self.model_name,
                device=device
            )
            
            self._model_loaded = True
            print(f"âœ… Model loaded successfully on {'GPU' if device == 0 else 'CPU'}")
            
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise RuntimeError(f"Model loading failed: {e}")
    
    async def detect(self, image_bytes: bytes) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ì˜ AI ìƒì„± ì—¬ë¶€ íƒì§€
        
        Returns:
            - is_ai_generated: AI ìƒì„± ì—¬ë¶€
            - confidence: í™•ì‹ ë„ (0.0 ~ 1.0)
            - raw_scores: ì›ë³¸ ì ìˆ˜
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            
            # ì¶”ë¡  ì‹¤í–‰ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None, 
                lambda: self.classifier(img)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            return self._parse_results(results)
            
        except Exception as e:
            return {
                "model_name": self.model_name,
                "is_ai_generated": False,
                "confidence": 0.0,
                "error": str(e),
                "raw_scores": None
            }
    
    def _parse_results(self, results: list) -> Dict[str, Any]:
        """ëª¨ë¸ ê²°ê³¼ íŒŒì‹±"""
        raw_scores = {r["label"]: r["score"] for r in results}
        
        # ëª¨ë¸ë³„ ë¼ë²¨ ë§¤í•‘
        model_config = self.AVAILABLE_MODELS.get(self.model_name, {})
        label_map = model_config.get("labels", {"artificial": "ai", "human": "real"})
        
        # AI ê´€ë ¨ ë¼ë²¨ ì ìˆ˜ í•©ì‚°
        ai_score = 0.0
        real_score = 0.0
        
        for label, score in raw_scores.items():
            label_lower = label.lower()
            
            # AI ê´€ë ¨ ë¼ë²¨
            if any(ai_key in label_lower for ai_key in ["artificial", "ai", "fake", "generated", "synthetic"]):
                ai_score += score
            # Real ê´€ë ¨ ë¼ë²¨
            elif any(real_key in label_lower for real_key in ["human", "real", "authentic", "natural"]):
                real_score += score
        
        # íŒì •
        is_ai = ai_score > real_score
        confidence = ai_score if is_ai else real_score
        
        return {
            "model_name": self.model_name,
            "is_ai_generated": is_ai,
            "confidence": round(confidence, 4),
            "raw_scores": raw_scores
        }
    
    async def detect_with_multiple_models(self, image_bytes: bytes) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ë¡œ ì•™ìƒë¸” íƒì§€ (ë” ì •í™•í•œ ê²°ê³¼)
        """
        results = {}
        
        for model_name in self.AVAILABLE_MODELS.keys():
            try:
                temp_detector = DetectionService(model_name)
                result = await temp_detector.detect(image_bytes)
                results[model_name] = result
            except Exception as e:
                results[model_name] = {"error": str(e)}
        
        # ì•™ìƒë¸” ê²°ê³¼ ê³„ì‚°
        ai_votes = 0
        total_confidence = 0.0
        valid_count = 0
        
        for model_name, result in results.items():
            if "error" not in result:
                valid_count += 1
                total_confidence += result["confidence"]
                if result["is_ai_generated"]:
                    ai_votes += 1
        
        ensemble_result = {
            "individual_results": results,
            "ensemble_verdict": ai_votes > valid_count / 2 if valid_count > 0 else False,
            "ensemble_confidence": total_confidence / valid_count if valid_count > 0 else 0.0,
            "models_used": valid_count
        }
        
        return ensemble_result
    
    def get_model_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "model_loaded": self._model_loaded,
            "available_models": list(self.AVAILABLE_MODELS.keys()),
            "device": "GPU" if torch.cuda.is_available() else "CPU"
        }
