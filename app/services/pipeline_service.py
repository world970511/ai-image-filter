"""
Pipeline Service
3ê°œ Layerë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© íŒì • ìˆ˜í–‰
"""

import uuid
import time
from datetime import datetime
from typing import Dict, Any, Optional

from app.services.hash_service import HashService
from app.services.metadata_service import MetadataService
from app.services.detection_service import DetectionService
from app.models.schemas import (
    AnalysisResult, 
    HashResult, 
    MetadataResult, 
    DetectionResult,
    VerdictType
)



class PipelineService:
    """3-Layer ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.hash_service = HashService()
        self.metadata_service = MetadataService()
        self.detection_service = DetectionService()
        
        # íŒì • ì„ê³„ê°’
        self.CONFIDENCE_THRESHOLD = 0.7
        self.AI_DETECTION_WEIGHT = 0.6
        self.METADATA_WEIGHT = 0.3
        self.HASH_WEIGHT = 0.1
    
    async def analyze_image(
        self, 
        image_bytes: bytes, 
        filename: str,
        skip_ai_detection: bool = False
    ) -> AnalysisResult:
        """
        ì´ë¯¸ì§€ ì¢…í•© ë¶„ì„ ì‹¤í–‰
        
        Args:
            image_bytes: ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë°ì´í„°
            filename: íŒŒì¼ëª…
            skip_ai_detection: AI íƒì§€ ìŠ¤í‚µ ì—¬ë¶€ (ë¹ ë¥¸ ë¶„ì„ìš©)
        """
        start_time = time.time()
        analysis_id = str(uuid.uuid4())
        layers_executed = []
        
        # ========== Layer 1: Hash Check ==========
        layer1_start = time.time()
        hash_data = self.hash_service.compute_hash(image_bytes)
        is_duplicate = await self.hash_service.check_duplicate(hash_data["md5"])
        
        hash_result = HashResult(
            md5=hash_data["md5"],
            sha256=hash_data["sha256"],
            perceptual_hash=hash_data.get("perceptual_hash"),
            is_duplicate=is_duplicate
        )
        layers_executed.append("hash_check")
        layer1_time = (time.time() - layer1_start) * 1000
        
        # ========== Layer 2: Metadata Analysis ==========
        layer2_start = time.time()
        metadata_data = self.metadata_service.analyze(image_bytes, filename)
        
        metadata_result = MetadataResult(
            has_c2pa=metadata_data.get("has_c2pa", False),
            c2pa_info=metadata_data.get("c2pa_info"),
            exif_data=metadata_data.get("exif_data"),
            ai_tool_signatures=metadata_data.get("ai_tool_signatures", []),
            software_used=metadata_data.get("software_used"),
            creation_date=metadata_data.get("creation_date")
        )
        layers_executed.append("metadata_analysis")
        layer2_time = (time.time() - layer2_start) * 1000
        
        # ========== Layer 3: AI Detection ==========
        detection_result = None
        layer3_time = 0
        
        if not skip_ai_detection:
            layer3_start = time.time()
            detection_data = await self.detection_service.detect(image_bytes)
            
            if "error" not in detection_data:
                detection_result = DetectionResult(
                    model_name=detection_data["model_name"],
                    is_ai_generated=detection_data["is_ai_generated"],
                    confidence=detection_data["confidence"],
                    raw_scores=detection_data.get("raw_scores")
                )
            layers_executed.append("ai_detection")
            layer3_time = (time.time() - layer3_start) * 1000
        
        # ========== ì¢…í•© íŒì • ==========
        verdict, confidence, reasoning = self._compute_verdict(
            hash_result=hash_result,
            metadata_result=metadata_result,
            detection_result=detection_result
        )
        
        total_time = (time.time() - start_time) * 1000
        
        # ê²°ê³¼ ìƒì„±
        result = AnalysisResult(
            id=analysis_id,
            filename=filename,
            analyzed_at=datetime.utcnow(),
            hash_result=hash_result,
            metadata_result=metadata_result,
            detection_result=detection_result,
            final_verdict=verdict,
            confidence_score=confidence,
            reasoning=reasoning,
            total_execution_time_ms=round(total_time, 2),
            layers_executed=layers_executed
        )
        

        
        return result
    
    def _compute_verdict(
        self,
        hash_result: HashResult,
        metadata_result: MetadataResult,
        detection_result: Optional[DetectionResult]
    ) -> tuple[VerdictType, float, str]:
        """
        ì¢…í•© íŒì • ê³„ì‚°
        
        ê°€ì¤‘ì¹˜ ê¸°ë°˜ íŒì •:
        - AI Detection: 60%
        - Metadata: 30%
        - Hash: 10%
        """
        scores = {
            "ai": 0.0,
            "real": 0.0
        }
        reasons = []
        
        # 1. Hash ê¸°ë°˜ íŒì • (ì¤‘ë³µì´ë©´ ì´ì „ íŒì • ì°¸ì¡° ê°€ëŠ¥)
        if hash_result.is_duplicate:
            reasons.append("âš ï¸ ì¤‘ë³µ ì´ë¯¸ì§€ ë°œê²¬")
        
        # 2. Metadata ê¸°ë°˜ íŒì •
        if metadata_result.ai_tool_signatures:
            tools = ", ".join(metadata_result.ai_tool_signatures)
            scores["ai"] += self.METADATA_WEIGHT
            reasons.append(f"ğŸ” AI ë„êµ¬ ì‹œê·¸ë‹ˆì²˜ ë°œê²¬: {tools}")
        
        if metadata_result.has_c2pa:
            reasons.append("ğŸ“œ C2PA Content Credentials ë°œê²¬")
            # C2PAê°€ ìˆìœ¼ë©´ ì¶”ê°€ ë¶„ì„ (AI ê´€ë ¨ assertion í™•ì¸)
            c2pa_info = metadata_result.c2pa_info or {}
            if c2pa_info.get("ai_related_assertions"):
                scores["ai"] += self.METADATA_WEIGHT * 0.5
                reasons.append("ğŸ¤– C2PAì— AI ìƒì„± ê´€ë ¨ ì •ë³´ í¬í•¨")
        
        # EXIFì— íŠ¹ì • íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì˜ì‹¬
        if not metadata_result.exif_data or len(metadata_result.exif_data) < 3:
            scores["ai"] += self.METADATA_WEIGHT * 0.3
            reasons.append("ğŸ“· EXIF ë©”íƒ€ë°ì´í„° ë¶€ì¡±/ì—†ìŒ (AI ì´ë¯¸ì§€ íŠ¹ì„±)")
        else:
            scores["real"] += self.METADATA_WEIGHT * 0.3
            reasons.append("ğŸ“· EXIF ë©”íƒ€ë°ì´í„° ì¡´ì¬")
        
        # 3. AI Detection ê¸°ë°˜ íŒì •
        if detection_result:
            if detection_result.is_ai_generated:
                scores["ai"] += self.AI_DETECTION_WEIGHT * detection_result.confidence
                reasons.append(
                    f"ğŸ¤– AI íƒì§€ ëª¨ë¸ íŒì •: AI ìƒì„± "
                    f"(í™•ì‹ ë„: {detection_result.confidence:.1%})"
                )
            else:
                scores["real"] += self.AI_DETECTION_WEIGHT * detection_result.confidence
                reasons.append(
                    f"âœ… AI íƒì§€ ëª¨ë¸ íŒì •: ì‹¤ì œ ì´ë¯¸ì§€ ê°€ëŠ¥ì„± "
                    f"(í™•ì‹ ë„: {detection_result.confidence:.1%})"
                )
        else:
            reasons.append("â­ï¸ AI íƒì§€ ìŠ¤í‚µë¨")
        
        # ìµœì¢… íŒì •
        total_score = scores["ai"] + scores["real"]
        if total_score == 0:
            verdict = VerdictType.UNCERTAIN
            confidence = 0.5
        else:
            ai_ratio = scores["ai"] / total_score if total_score > 0 else 0.5
            
            if ai_ratio >= self.CONFIDENCE_THRESHOLD:
                verdict = VerdictType.AI_GENERATED
                confidence = ai_ratio
            elif ai_ratio <= (1 - self.CONFIDENCE_THRESHOLD):
                verdict = VerdictType.LIKELY_REAL
                confidence = 1 - ai_ratio
            else:
                verdict = VerdictType.UNCERTAIN
                confidence = 0.5 + abs(ai_ratio - 0.5)
        
        reasoning = " | ".join(reasons)
        
        return verdict, round(confidence, 4), reasoning
    

