"""
Pipeline Service
3ê°œ Layerë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© íŒì • ìˆ˜í–‰
"""

import uuid
import time
from datetime import datetime
from typing import Dict, Any, Optional

from app.services.hash_service import HashService
from app.services.metadata_service import MetadataService
from app.services.detection_service import DetectionService
from app.models.schemas import (
    AnalysisResult, 
    HashResult, 
    MetadataResult, 
    DetectionResult,
    VerdictType
)

class PipelineService:
    """3-Layer ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        db_vectors_path: str = './data/ai_dinohashes.npy',
        metadata_path: str = './data/ai_metadata.csv',
        similarity_threshold: float = 0.85
    ):
        """
        PipelineService ì´ˆê¸°í™”

        Args:
            db_vectors_path: AI ì´ë¯¸ì§€ ë²¡í„° íŒŒì¼ ê²½ë¡œ
            metadata_path: AI ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            similarity_threshold: DinoV2 ìœ ì‚¬ë„ ìž„ê³„ê°’
        """
        self.hash_service = HashService(
            db_vectors_path=db_vectors_path,
            metadata_path=metadata_path,
            threshold=similarity_threshold if similarity_threshold else 0.85
        )
        self.metadata_service = MetadataService()
        self.detection_service = DetectionService()

        # íŒì • ìž„ê³„ê°’
        self.CONFIDENCE_THRESHOLD = 0.7
        self.AI_DETECTION_WEIGHT = 0.3
        self.METADATA_WEIGHT = 0.4
        self.HASH_WEIGHT = 0.3
    
    async def analyze_image(
        self, 
        image_bytes: bytes, 
        filename: str,
    ) -> AnalysisResult:
        """
        ì´ë¯¸ì§€ ì¢…í•© ë¶„ì„ ì‹¤í–‰
        
        Args:
            image_bytes: ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë°ì´í„°
            filename: íŒŒì¼ëª…
        """
        start_time = time.time()
        analysis_id = str(uuid.uuid4())
        layers_executed = []
        
        # ========== Layer 1: Hash Check ==========
        layer1_start = time.time()
        hash_data = self.hash_service.compute_hash(image_bytes)
        
        hash_result = HashResult(
            is_ai=hash_data["is_ai"],
            similarity=hash_data["similarity"],
        )
        layers_executed.append("hash_check")
        layer1_time = (time.time() - layer1_start) * 1000
        
        # ========== Layer 2: Metadata Analysis ==========
        layer2_start = time.time()
        metadata_data = self.metadata_service.analyze(image_bytes, filename)
        
        metadata_result = MetadataResult(
            has_c2pa=metadata_data.get("has_c2pa", False),
            c2pa_info=metadata_data.get("c2pa_info"),
            exif_data=metadata_data.get("exif_data"),
            ai_tool_signatures=metadata_data.get("ai_tool_signatures", []),
            software_used=metadata_data.get("software_used"),
            creation_date=metadata_data.get("creation_date"),
            exif_authenticity_score=metadata_data.get("exif_authenticity_score", 0.0),
            exif_inconsistencies=metadata_data.get("exif_inconsistencies", [])
        )
        layers_executed.append("metadata_analysis")
        layer2_time = (time.time() - layer2_start) * 1000
        
        # ========== Layer 3: AI Detection ==========
        detection_result = None
        layer3_time = 0
        
        layer3_start = time.time()
        detection_data = await self.detection_service.detect(image_bytes)
            
        if "error" not in detection_data:
            detection_result = DetectionResult(
                model_name=detection_data["model_name"],
                is_ai_generated=detection_data["is_ai_generated"],
                confidence=detection_data["confidence"],
                raw_scores=detection_data.get("raw_scores")
                )
        layers_executed.append("ai_detection")
        layer3_time = (time.time() - layer3_start) * 1000
        
        # ========== ì¢…í•© íŒì • ==========
        verdict, confidence, reasoning = self._compute_verdict(
            hash_result=hash_result,
            metadata_result=metadata_result,
            detection_result=detection_result
        )
        
        total_time = (time.time() - start_time) * 1000
        
        # ê²°ê³¼ ìƒì„±
        result = AnalysisResult(
            id=analysis_id,
            filename=filename,
            analyzed_at=datetime.utcnow(),
            hash_result=hash_result,
            metadata_result=metadata_result,
            detection_result=detection_result,
            final_verdict=verdict,
            confidence_score=confidence,
            reasoning=reasoning,
            total_execution_time_ms=round(total_time, 2),
            layers_executed=layers_executed
        )
        return result
    
    def _compute_verdict(
        self,
        hash_result: HashResult,
        metadata_result: MetadataResult,
        detection_result: Optional[DetectionResult]
    ) -> tuple[VerdictType, float, str]:
        """
        ì¢…í•© íŒì • ê³„ì‚°

        ê°€ì¤‘ì¹˜ ê¸°ë°˜ íŒì •:
        - Hash: 30% (DinoV2 ìœ ì‚¬ë„, ì ì§„ì  ê³„ì‚°)
        - Metadata: 40% (EXIF ì§„ìœ„ì„± + C2PA/ì‹œê·¸ë‹ˆì²˜)
        - AI Detection: 30% (HuggingFace ëª¨ë¸)

        Hash ì ì§„ì  ê³„ì‚°:
        - 85% ì´ìƒ: AI ì ìˆ˜ (ê°•ë„ì— ë¹„ë¡€)
        - 70-85%: ë¶ˆí™•ì‹¤ ì˜ì—­ (ì–‘ìª½ ì ìˆ˜ ë¶„ë°°)
        - 70% ë¯¸ë§Œ: Real ì ìˆ˜
        """
        scores = {
            "ai": 0.0,
            "real": 0.0
        }
        reasons = []
        
        # 1. Hash ê¸°ë°˜ íŒì • (DinoV2 ë²¡í„° ìœ ì‚¬ë„) - ì ì§„ì  ì ìˆ˜ ê³„ì‚°
        similarity = hash_result.similarity

        if similarity >= 0.85:
            # 85% ì´ìƒ: í™•ì‹¤í•œ AI ì´ë¯¸ì§€ (ìž„ê³„ê°’ ì´ìƒ)
            ai_score = self.HASH_WEIGHT * min((similarity - 0.85) / 0.15 + 0.5, 1.0)
            scores["ai"] += ai_score
            reasons.append(
                f"âš ï¸ AI ì´ë¯¸ì§€ DBì™€ {'ë§¤ì¹­ë¨' if hash_result.is_ai else 'ë†’ì€ ìœ ì‚¬ë„'} "
                f"(ìœ ì‚¬ë„: {similarity:.1%})"
            )
        elif similarity >= 0.70:
            # 70-85%: ë¶ˆí™•ì‹¤í•œ ì˜ì—­ (ìœ ì‚¬í•˜ì§€ë§Œ í™•ì‹  ë¶€ì¡±)
            # ìœ ì‚¬ë„ì— ë¹„ë¡€í•˜ì—¬ ì ìˆ˜ ë¶„ë°°
            uncertainty = (0.85 - similarity) / 0.15
            ai_portion = self.HASH_WEIGHT * 0.5 * (1 - uncertainty)
            real_portion = self.HASH_WEIGHT * 0.5 * uncertainty
            scores["ai"] += ai_portion
            scores["real"] += real_portion
            reasons.append(
                f"âš ï¸ AI ì´ë¯¸ì§€ DBì™€ ì¤‘ê°„ ìœ ì‚¬ë„ "
                f"(ìœ ì‚¬ë„: {similarity:.1%}, ë¶ˆí™•ì‹¤)"
            )
        else:
            # 70% ë¯¸ë§Œ: ì‹¤ì œ ì´ë¯¸ì§€ ê°€ëŠ¥ì„±
            real_score = self.HASH_WEIGHT * 0.5
            scores["real"] += real_score
            reasons.append(
                f"âœ“ AI ì´ë¯¸ì§€ DBì™€ ë‚®ì€ ìœ ì‚¬ë„ "
                f"(ìµœëŒ€ ìœ ì‚¬ë„: {similarity:.1%})"
            )
        
        # 2. Metadata ê¸°ë°˜ íŒì •
        # 2-1. AI ë„êµ¬ ì‹œê·¸ë‹ˆì²˜ (ê°•ë ¥í•œ AI ì¦ê±°)
        if metadata_result.ai_tool_signatures:
            tools = ", ".join(metadata_result.ai_tool_signatures)
            scores["ai"] += self.METADATA_WEIGHT * 0.4
            reasons.append(f"ðŸ” AI ë„êµ¬ ì‹œê·¸ë‹ˆì²˜ ë°œê²¬: {tools}")

        # 2-2. C2PA ë¶„ì„
        if metadata_result.has_c2pa:
            c2pa_info = metadata_result.c2pa_info or {}
            if c2pa_info.get("ai_related_assertions"):
                scores["ai"] += self.METADATA_WEIGHT * 0.2
                reasons.append("ðŸ¤– C2PAì— AI ìƒì„± ê´€ë ¨ ì •ë³´ í¬í•¨")
            else:
                # C2PAê°€ ìžˆì§€ë§Œ AI ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì‹¤ì œ ì´ë¯¸ì§€ ê°€ëŠ¥ì„±
                scores["real"] += self.METADATA_WEIGHT * 0.15
                reasons.append("ðŸ“œ C2PA Content Credentials ì¡´ìž¬ (AI ê´€ë ¨ ì •ë³´ ì—†ìŒ)")

        # 2-3. EXIF ì§„ìœ„ì„± ì ìˆ˜ í™œìš© (ìƒˆë¡œ ì¶”ê°€ëœ í•µì‹¬ ê¸°ëŠ¥)
        exif_score = metadata_result.exif_authenticity_score

        if exif_score >= 0.7:
            # ë†’ì€ EXIF ì§„ìœ„ì„± = ì‹¤ì œ ì¹´ë©”ë¼ë¡œ ì´¬ì˜
            scores["real"] += self.METADATA_WEIGHT * 0.35 * exif_score
            reasons.append(f"ðŸ“· EXIF ì§„ìœ„ì„± ë†’ìŒ (ì ìˆ˜: {exif_score:.2f}) - ì‹¤ì œ ì¹´ë©”ë¼ ì´¬ì˜ ê°€ëŠ¥ì„±")
        elif exif_score >= 0.3:
            # ì¤‘ê°„ ìˆ˜ì¤€
            scores["real"] += self.METADATA_WEIGHT * 0.15 * exif_score
            reasons.append(f"ðŸ“· EXIF ë°ì´í„° ì¡´ìž¬ (ì§„ìœ„ì„±: {exif_score:.2f})")
        else:
            # ë‚®ì€ EXIF ì§„ìœ„ì„± = AI ìƒì„± ì˜ì‹¬
            scores["ai"] += self.METADATA_WEIGHT * 0.25
            reasons.append(f"âš ï¸ EXIF ì§„ìœ„ì„± ë‚®ìŒ (ì ìˆ˜: {exif_score:.2f}) - AI ìƒì„± ì˜ì‹¬")

        # 2-4. EXIF ë¹„ì •ìƒ íŒ¨í„´ íƒì§€
        if metadata_result.exif_inconsistencies:
            inconsistency_weight = min(len(metadata_result.exif_inconsistencies) * 0.05, 0.15)
            scores["ai"] += self.METADATA_WEIGHT * inconsistency_weight
            inconsistency_msgs = {
                "editing_software_without_camera": "íŽ¸ì§‘ SWë§Œ ì¡´ìž¬",
                "perfect_square_ai_resolution": "AI ìƒì„± í•´ìƒë„",
                "unrealistic_aperture": "ë¹„í˜„ì‹¤ì  ì´¬ì˜ê°’",
                "missing_datetime_original": "ì›ë³¸ ì‹œê°„ ëˆ„ë½"
            }
            detected = [inconsistency_msgs.get(inc, inc) for inc in metadata_result.exif_inconsistencies]
            reasons.append(f"âš ï¸ EXIF ë¹„ì •ìƒ íŒ¨í„´: {', '.join(detected)}")
        
        # 3. AI Detection ê¸°ë°˜ íŒì •
        if detection_result:
            if detection_result.is_ai_generated:
                scores["ai"] += self.AI_DETECTION_WEIGHT * detection_result.confidence
                reasons.append(
                    f"ðŸ¤– AI íƒì§€ ëª¨ë¸ íŒì •: AI ìƒì„± "
                    f"(í™•ì‹ ë„: {detection_result.confidence:.1%})"
                )
            else:
                scores["real"] += self.AI_DETECTION_WEIGHT * detection_result.confidence
                reasons.append(
                    f"âœ… AI íƒì§€ ëª¨ë¸ íŒì •: ì‹¤ì œ ì´ë¯¸ì§€ ê°€ëŠ¥ì„± "
                    f"(í™•ì‹ ë„: {detection_result.confidence:.1%})"
                )
        else:
            reasons.append("â­ï¸ AI íƒì§€ ìŠ¤í‚µë¨")
        
        # ìµœì¢… íŒì •
        total_score = scores["ai"] + scores["real"]
        if total_score == 0:
            verdict = VerdictType.UNCERTAIN
            confidence = 0.5
        else:
            ai_ratio = scores["ai"] / total_score if total_score > 0 else 0.5
            
            if ai_ratio >= self.CONFIDENCE_THRESHOLD:
                verdict = VerdictType.AI_GENERATED
                confidence = ai_ratio
            elif ai_ratio <= (1 - self.CONFIDENCE_THRESHOLD):
                verdict = VerdictType.LIKELY_REAL
                confidence = 1 - ai_ratio
            else:
                verdict = VerdictType.UNCERTAIN
                confidence = 0.5 + abs(ai_ratio - 0.5)
        
        reasoning = " | ".join(reasons)
        
        return verdict, round(confidence, 4), reasoning
    

